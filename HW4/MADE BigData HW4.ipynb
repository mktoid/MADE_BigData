{
  "metadata": {
    "name": "MADE BigData HW4",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## По данным TripAdvisor hotel reviews посчитать Tf-Idf с помощью Spark DataFrame / Dataset API без использования Spark ML"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\nwget https://raw.githubusercontent.com/mktoid/MADE_BigData/main/HW4/tripadvisor_hotel_reviews.csv"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val spark \u003d SparkSession.builder()\r\n    // адрес мастера\r\n    .master(\"local[*]\")\r\n    // имя приложения в интерфейсе спарка\r\n    .appName(\"made-demo\")\r\n//     .config(\"spark.executor.memory\",  \"2g\")\r\n//     .config(\"spark.executor.cores\", \"2\")\r\n//     .config(\"spark.driver.memory\", \"2g\")\r\n    .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var df \u003d spark.read\r\n    .option(\"header\", \"true\")\r\n    .option(\"inferSchema\", \"true\")\r\n    .option(\"sep\", \",\")\r\n    .csv(\"tripadvisor_hotel_reviews.csv\")\r\n    \r\ndf.show(3, 256)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Привести все к одному регистру\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df \u003d df.select(lower(col(\"Review\")) as \"Review\")\ndf.show(3, 256)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## Удалить все спецсимволы"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var dfClean \u003d df.select(split(col(\"Review\"), \"[^a-zA-z0-9*\u0027]{1,}\") as \"Review\").as[Seq[String]]\ndfClean.show(3, 256)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Посчитать частоту слова в предложении"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val wordFrequencyInSencence \u003d dfClean.rdd.map(row \u003d\u003e {\n    val wordFrequency \u003d row.foldLeft(new HashMap[String, Int]()) {\n        (map, word) \u003d\u003e {\n            map +\u003d word -\u003e (map.getOrElse(word, 0) + 1)\n            map\n        }\n    }\n    wordFrequency\n})\n\nwordFrequencyInSencence.toDF.show(3, 256)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Посчитать количество документов со словом"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val numDocsWithWord \u003d wordFrequencyInSencence\n    .flatMap(_.keySet)\n    .map((_, 1))\n    .reduceByKey(_ + _)\n    \nnumDocsWithWord.toDF.show(3, 256)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Взять только 100 самых встречаемых"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val topWords \u003d numDocsWithWord.top(100)(Ordering.by[(String, Int), Int](_._2))"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val docsCount \u003d numDocsWithWord.count().toDouble\nval idfs \u003d topWords.map{case (word, count) \u003d\u003e\n    (word, math.log(docsCount / count))    \n}.toMap"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val wordToIndex \u003d idfs.keys.zipWithIndex.toMap\nval indexToWord \u003d idfs.keys.map(key \u003d\u003e (wordToIndex(key), key)).toMap"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Сджойнить две полученные таблички и посчитать Tf-Idf (только для слов из предыдущего пункта)\n## Запайвотить табличку"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val tfIdf \u003d wordFrequencyInSencence.map(wordFrequency \u003d\u003e {\n    val wordCountInReview \u003d wordFrequency.values.sum\n    val wordScore \u003d wordFrequency.filter{\n        case (word, freq) \u003d\u003e wordToIndex.contains(word)\n    }.map{\n        case (word, freq) \u003d\u003e (wordToIndex(word), idfs(word) * wordFrequency(word) / wordCountInReview)\n    }.toSeq\n    Vectors.sparse(wordToIndex.size, wordScore).toArray\n})\n\ntfIdf.toDF.show(3, 256)"
    }
  ]
}